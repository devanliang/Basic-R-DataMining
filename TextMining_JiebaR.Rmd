---
title: "TextMining_JiebaR"
output: html_document
---

```{R}
#通過CRAN安裝
install.packages("jiebaR")
#新建立一個分詞器
library(jiebaR)
seg_worker = worker()
# 分詞
segment("我是一段文本", seg_worker)

# 安裝library
install.packages("devtools")
install.packages("tidyRSS")
install.packages("XML")
install.packages("RCurl")
install.packages("plyr")
install.packages("wordcloud")
install.packages("wordcloud2")

# 引入library
library(tidyRSS)
library(XML)
library(RCurl)
library(jiebaR)
library(stringr)
library(plyr)
library(wordcloud)
library(wordcloud2)

library(tidyRSS)
rss_url <- 'https://udn.com/rssfeed/news/2/6645?ch=news'
rss <- tidyRSS::tidyfeed(feed = rss_url)
rss$feed_title # RSS標題
rss$feed_link # RSS超連結
rss$feed_description # 說明
rss$feed_language # 語系
rss$item_title # 文章標題
rss$item_link # 文章超連結
```

# 爬蟲抓Udn文字
# 準備request
```{r}
ua <- "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36"
myHttpHeader <- c(
  "User-Agent"=ua,
  "accept"="text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8",
  "accept-Language"="zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7",
  "accept-encoding"="gzip, deflate, br",
  "Connection"="keep-alive",
  "cache-control"="no-cache",
  "Accept-Charset"="UTF8,utf-8;q=0.7,*;q=0.7"
)
curl_handle <- getCurlHandle()
curlSetOpt(cookiejar="cookies.txt", useragent = ua, followlocation = TRUE, curl=curl_handle)
```

```{R}
# 爬蟲程式
data <- list()
i <- 1
for(link in rss$item_link){
  print(paste(i, link, sep=","))
  html_doc <- htmlParse(getURL(link, curl = curl_handle), encoding = "UTF-8")
  article_item <- xpathSApply(html_doc, '//*[@id="story_body_content"]//p', xmlValue)
  article_item <- gsub("\\s+", "", article_item)
  article_item <- gsub(" $", "", article_item)
  data[i] <- article_item
  i <- i+1
  t <- sample(2:5,1)
  Sys.sleep(t)
}
data <- unlist(data)


Sys.getlocale()
Sys.setlocale(category='LC_ALL', locale='C')


# 進行分詞
cutter = worker()
i <- 1
for( link in rss$item_link ){
data[i] <- article_item
}
data <- unlist(data)

# 製作分詞器
cutter = worker(stop_word = "D:/DataMining/stopword.txt")
seg_words <- cutter <= data

#方法一
library(plyr)
table_words <- count(seg_words)#計算字頻

#方法二 在R使用SQL語法
install.packages("sqldf")
library(sqldf)
seg_words1 <- as.data.frame(x = seg_words)
table_words <- sqldf("SELECT seg_words,count(*) FROM seg_words1 group by
seg_words")

# 繪製文字雲
library(wordcloud2)
wordcloud2(table_words, size = 0.2, fontFamily = "微软雅黑", 
           color = "random-light", backgroundColor = "grey")
```